# -*- coding: utf-8 -*-
"""EffNet_con_augmentation.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1UmtRWzSGn2rt_mWffOa0NFVGCfDNwkO-
"""

# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only "../input/" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using "Save & Run All"
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session

# Commented out IPython magic to ensure Python compatibility.
from google.colab import drive
drive.mount('/gdrive')
# %cd /gdrive/My Drive/ANNDL

# Fix randomness and hide warnings
seed = 42

import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'
os.environ['PYTHONHASHSEED'] = str(seed)
os.environ['MPLCONFIGDIR'] = os.getcwd()+'/configs/'

import warnings
warnings.simplefilter(action='ignore', category=FutureWarning)
warnings.simplefilter(action='ignore', category=Warning)

import numpy as np
np.random.seed(seed)

import logging

import random
random.seed(seed)

# Import tensorflow
import tensorflow as tf
from tensorflow import keras as tfk
from tensorflow.keras import layers as tfkl
tf.autograph.set_verbosity(0)
tf.get_logger().setLevel(logging.ERROR)
tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)
tf.random.set_seed(seed)
tf.compat.v1.set_random_seed(seed)
print(tf.__version__)

# Import other libraries
import cv2
from tensorflow.keras.applications.efficientnet_v2 import preprocess_input
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split

#load data
data = np.load('cleaned_dataset.npz', allow_pickle=True)


X = data["data"]
Y = data["labels"]
X.shape, Y.shape

X = (X/255).astype(np.float32)

#change labels in 0 for healthy and 1 for unhealthy

for i in range(Y.size):
  if Y[i] == "healthy":
    Y[i] = 0
  else:
    Y[i] = 1

X, Y

# Number of images to display
num_img = 30

# Create subplots for displaying pictures
fig, axes = plt.subplots(2, num_img//2, figsize=(20, 9))
for i in range(num_img):
    ax = axes[i%2, i%num_img//2]
    ax.imshow(np.clip(X[i], 0, 255))  # Display clipped item images
    ax.axis('off')
plt.tight_layout()
plt.show()

#one-hot encoding
y = tfk.utils.to_categorical(Y,2)

# Split data into train_val and test sets
X_train, X_val, y_train, y_val = train_test_split(X, y, random_state=seed, test_size=260, stratify=np.argmax(y,axis=1))


# Print shapes of the datasets
print(f"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}")
print(f"X_val shape: {X_val.shape}, y_val shape: {y_val.shape}")

# Create ConvNeXtXLarge model with specified settings
mobile = tf.keras.applications.ConvNeXtXLarge(
    input_shape=(96, 96, 3),
    include_top=False,
    weights="imagenet",
    pooling='avg'
)

mobile.summary()

# Use the supernet as feature extractor, i.e. freeze all its weigths
mobile.trainable = False


# Create an input layer with shape (96, 96, 3)
inputs = tfk.Input(shape=(96, 96, 3))

#inputs = tfkl.GaussianNoise(stddev=0.1)(inputs)

preprocessing = tf.keras.Sequential([
        tfkl.RandomTranslation(height_factor=(-0.2, 0.3), width_factor=(-0.2, 0.3)),
        tfkl.RandomFlip(mode="horizontal_and_vertical", seed=None),
    ], name='preprocessing')

preprocessing = preprocessing(inputs)

# Connect ConvNeXtXLarge to the input
x = mobile(preprocessing)

#x = tfkl.Dense(128, activation='gelu')(x)

#x = tfkl.BatchNormalization(name='BatchNorm0')(x)

x = tfkl.Dense(64, activation='gelu')(x)

# Initial convolution with batch normalization and activation
x = tfkl.BatchNormalization(name='BatchNorm1')(x)

x = tfkl.Dropout(0.3)(x)
#x = tfkl.GlobalAveragePooling2D(name='GlobalAveragePooling')(x)
# Add a Dense layer with 2 units and softmax activation as the classifier
outputs = tfkl.Dense(2, activation='softmax')(x)



# Create a Model connecting input and output
tl_model = tfk.Model(inputs=inputs, outputs=outputs, name='model')

# Compile the model with Categorical Cross-Entropy loss and Adam optimizer
tl_model.compile(loss=tfk.losses.CategoricalCrossentropy(), optimizer=tfk.optimizers.AdamW(1e-4,weight_decay=5e-4), metrics=['accuracy'])

# Display model summary
tl_model.summary()

# Train the model
tl_history = tl_model.fit(
    x = X_train*255,
    y = y_train,
    batch_size = 64,
    epochs = 100,
    validation_data = (X_val*255, y_val), # We need to apply the preprocessing thought for the MobileNetV2 network
    callbacks = [tfk.callbacks.EarlyStopping(monitor='val_accuracy', mode='max', patience=20, restore_best_weights=True),
                tfk.callbacks.ReduceLROnPlateau(monitor="val_accuracy", factor=0.1, patience=20, min_lr=1e-6, mode='max')]
).history

# Save the best model
tl_model.save('ConvNetXL_con_aug_all')
del tl_model

"""**FINE TUNING**"""

# Re-load the model after transfer learning
ft_model = tfk.models.load_model('ConvNetXL_con_aug_all')
ft_model.summary()

# Set all  layers as trainable
layers = ft_model.get_layer('convnext_xlarge')


# Use the supernet as feature extractor, i.e. freeze all its weigths
layers.trainable = True


# Freeze first N layers,
N = 270
for i, layer in enumerate(ft_model.get_layer('convnext_xlarge').layers[:N]):
  layer.trainable=False
for i, layer in enumerate(ft_model.get_layer('convnext_xlarge').layers):
   print(i, layer.name, layer.trainable)
ft_model.summary()


# Compile the model with Categorical Cross-Entropy loss and Adam optimizer
ft_model.compile(loss=tfk.losses.CategoricalCrossentropy(), optimizer=tfk.optimizers.AdamW(1e-4,weight_decay=5e-4), metrics=['accuracy'])

# Display model summary
ft_model.summary()

# Fine-tune the model
ft_history = ft_model.fit(
    x = X_train*255, # We need to apply the preprocessing thought for the MobileNetV2 network
    y = y_train,
    batch_size = 64,
    epochs = 100,
    validation_data = (X_val*255, y_val), # We need to apply the preprocessing thought for the MobileNetV2 network
    callbacks = [tfk.callbacks.EarlyStopping(monitor='val_accuracy', mode='max', patience=15, restore_best_weights=True),
                tfk.callbacks.ReduceLROnPlateau(monitor="val_accuracy", factor=0.1, patience=15, min_lr=1e-5, mode='max')
]
).history

# Save the best model
ft_model.save('ConvNetXL_con_aug_all_finetuning')
del ft_model