{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Fix randomness and hide warnings\nseed = 42\n\nimport os\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\nos.environ['PYTHONHASHSEED'] = str(seed)\nos.environ['MPLCONFIGDIR'] = os.getcwd()+'/configs/'\n\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nwarnings.simplefilter(action='ignore', category=Warning)\n\nimport numpy as np\nnp.random.seed(seed)\n\nimport logging\n\nimport random\nrandom.seed(seed)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-11-09T17:34:56.089079Z","iopub.execute_input":"2023-11-09T17:34:56.089349Z","iopub.status.idle":"2023-11-09T17:34:56.101709Z","shell.execute_reply.started":"2023-11-09T17:34:56.089323Z","shell.execute_reply":"2023-11-09T17:34:56.100957Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# Import tensorflow\nimport tensorflow as tf\nfrom tensorflow import keras as tfk\nfrom tensorflow.keras import layers as tfkl\n# tf.autograph.set_verbosity(0)\ntf.get_logger().setLevel(logging.ERROR)\ntf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\ntf.random.set_seed(seed)\ntf.compat.v1.set_random_seed(seed)\nprint(tf.__version__)","metadata":{"execution":{"iopub.status.busy":"2023-11-09T17:34:56.107048Z","iopub.execute_input":"2023-11-09T17:34:56.107683Z","iopub.status.idle":"2023-11-09T17:35:10.815662Z","shell.execute_reply.started":"2023-11-09T17:34:56.107656Z","shell.execute_reply":"2023-11-09T17:35:10.814668Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"2.12.0\n","output_type":"stream"}]},{"cell_type":"code","source":"# Import other libraries\nimport cv2\nfrom tensorflow.keras.applications.efficientnet_v2 import preprocess_input\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split","metadata":{"execution":{"iopub.status.busy":"2023-11-09T17:35:10.817203Z","iopub.execute_input":"2023-11-09T17:35:10.817755Z","iopub.status.idle":"2023-11-09T17:35:11.719170Z","shell.execute_reply.started":"2023-11-09T17:35:10.817727Z","shell.execute_reply":"2023-11-09T17:35:11.718281Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"#load data\ndata = np.load('/kaggle/input/clean-dataset/clean_dataset.npz', allow_pickle=True)\n\n\nX = data[\"data\"]\nY = data[\"labels\"]\nX.shape, Y.shape","metadata":{"execution":{"iopub.status.busy":"2023-11-09T17:35:11.720376Z","iopub.execute_input":"2023-11-09T17:35:11.720715Z","iopub.status.idle":"2023-11-09T17:35:16.615891Z","shell.execute_reply.started":"2023-11-09T17:35:11.720688Z","shell.execute_reply":"2023-11-09T17:35:16.614940Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"((5004, 96, 96, 3), (5004,))"},"metadata":{}}]},{"cell_type":"code","source":"X = (X/255).astype(np.float32)\n\n#change labels in 0 for healthy and 1 for unhealthy\n\nfor i in range(Y.size):\n  if Y[i] == \"healthy\":\n    Y[i] = 0\n  else:\n    Y[i] = 1\n\nY","metadata":{"execution":{"iopub.status.busy":"2023-11-09T17:35:16.618374Z","iopub.execute_input":"2023-11-09T17:35:16.618704Z","iopub.status.idle":"2023-11-09T17:35:16.988083Z","shell.execute_reply.started":"2023-11-09T17:35:16.618679Z","shell.execute_reply":"2023-11-09T17:35:16.987225Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"array([0, 0, 0, ..., 0, 0, 0], dtype=object)"},"metadata":{}}]},{"cell_type":"code","source":"#train, validation, test split (80,10,10)\n\n#one-hot encoding\ny = tfk.utils.to_categorical(Y,2)\n\n# Split data into train_val and test sets\nX_train_val, X_test, y_train_val, y_test = train_test_split(X, y, random_state=seed, test_size=520, stratify=np.argmax(y,axis=1))\n\n# Further split train_val into train and validation sets\nX_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, random_state=seed, test_size=520, stratify=np.argmax(y_train_val,axis=1))\n\n# Print shapes of the datasets\nprint(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\nprint(f\"X_val shape: {X_val.shape}, y_val shape: {y_val.shape}\")\nprint(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")","metadata":{"execution":{"iopub.status.busy":"2023-11-09T17:35:16.989001Z","iopub.execute_input":"2023-11-09T17:35:16.989256Z","iopub.status.idle":"2023-11-09T17:35:17.315184Z","shell.execute_reply.started":"2023-11-09T17:35:16.989233Z","shell.execute_reply":"2023-11-09T17:35:17.314222Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"X_train shape: (3964, 96, 96, 3), y_train shape: (3964, 2)\nX_val shape: (520, 96, 96, 3), y_val shape: (520, 2)\nX_test shape: (520, 96, 96, 3), y_test shape: (520, 2)\n","output_type":"stream"}]},{"cell_type":"code","source":"from tensorflow.keras.applications import EfficientNetV2M\nmodel = EfficientNetV2M(\n    include_top=False,\n    weights=\"imagenet\",\n    input_shape=(96,96,3),\n    pooling='avg',\n)","metadata":{"execution":{"iopub.status.busy":"2023-11-09T17:35:17.316608Z","iopub.execute_input":"2023-11-09T17:35:17.316974Z","iopub.status.idle":"2023-11-09T17:35:31.938169Z","shell.execute_reply.started":"2023-11-09T17:35:17.316940Z","shell.execute_reply":"2023-11-09T17:35:31.937357Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/efficientnet_v2/efficientnetv2-m_notop.h5\n214201816/214201816 [==============================] - 1s 0us/step\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Model with data augmentation","metadata":{}},{"cell_type":"code","source":"# Use the supernet as feature extractor, i.e. freeze all its weigths\nmodel.trainable = False\n\n# Create an input layer with shape (96, 96, 3)\ninputs = tfk.Input(shape=(96, 96, 3))\n\npreprocessing = tf.keras.Sequential([\n    tfkl.RandomTranslation(height_factor=(-0.2, 0.3), width_factor=(-0.2, 0.3)),\n    tfkl.RandomFlip(mode=\"horizontal_and_vertical\", seed=None),\n], name='preprocessing')\n\npreprocessing = preprocessing(inputs)\n\n\n# Connect EfficientNet to the input\nx = model(preprocessing)\n\n# Add a Dense layer with 2 units and softmax activation as the classifier\noutputs = tfkl.Dense(2, activation='softmax')(x)\n\n# Create a Model connecting input and output\ntl_model = tfk.Model(inputs=inputs, outputs=outputs, name='model')\n\n# Define optimizer, loss, and metrics\n# AdamW is an Adam optimizer which applies weight_decay to network layers,\n# i.e it's another way to apply l2 regularization to the whole network\noptimizer = tfk.optimizers.AdamW(1e-4, weight_decay=5e-4)\nloss = tfk.losses.CategoricalCrossentropy()\nmetrics = ['accuracy']\n\n# Compile the model with Categorical Cross-Entropy loss and Adam optimizer\ntl_model.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n\n# Display model summary\ntl_model.summary()","metadata":{"execution":{"iopub.status.busy":"2023-11-09T16:03:47.883616Z","iopub.execute_input":"2023-11-09T16:03:47.884369Z","iopub.status.idle":"2023-11-09T16:03:50.582626Z","shell.execute_reply.started":"2023-11-09T16:03:47.884331Z","shell.execute_reply":"2023-11-09T16:03:50.581656Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Model: \"model\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_2 (InputLayer)        [(None, 96, 96, 3)]       0         \n                                                                 \n preprocessing (Sequential)  (None, 96, 96, 3)         0         \n                                                                 \n efficientnetv2-m (Functiona  (None, 1280)             53150388  \n l)                                                              \n                                                                 \n dense (Dense)               (None, 2)                 2562      \n                                                                 \n=================================================================\nTotal params: 53,152,950\nTrainable params: 2,562\nNon-trainable params: 53,150,388\n_________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Model without data augmentation","metadata":{}},{"cell_type":"code","source":"# Use the supernet as feature extractor, i.e. freeze all its weigths\nmodel.trainable = False\n\n# Create an input layer with shape (96, 96, 3)\ninputs = tfk.Input(shape=(96, 96, 3))\n\n\n# Connect EfficientNet to the input\nx = model(inputs)\n\n# Add a Dense layer with 2 units and softmax activation as the classifier\noutputs = tfkl.Dense(2, activation='softmax')(x)\n\n# Create a Model connecting input and output\ntl_model = tfk.Model(inputs=inputs, outputs=outputs, name='model')\n\n# Define optimizer, loss, and metrics\n# AdamW is an Adam optimizer which applies weight_decay to network layers,\n# i.e it's another way to apply l2 regularization to the whole network\noptimizer = tfk.optimizers.AdamW(1e-4, weight_decay=5e-4)\nloss = tfk.losses.CategoricalCrossentropy()\nmetrics = ['accuracy']\n\n# Compile the model with Categorical Cross-Entropy loss and Adam optimizer\ntl_model.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n\n# Display model summary\ntl_model.summary()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model with added dense + batch norm + dropout + augmentation","metadata":{}},{"cell_type":"code","source":"# Use the supernet as feature extractor, i.e. freeze all its weigths\nmodel.trainable = False\n\n# Create an input layer with shape (96, 96, 3)\ninputs = tfk.Input(shape=(96, 96, 3))\n\npreprocessing = tf.keras.Sequential([\n    tfkl.RandomTranslation(height_factor=(-0.2, 0.3), width_factor=(-0.2, 0.3)),\n    tfkl.RandomFlip(mode=\"horizontal_and_vertical\", seed=None),\n], name='preprocessing')\n\npreprocessing = preprocessing(inputs)\n\n\n# Connect EfficientNet to the input\nx = model(preprocessing)\n\nx = tfkl.Dense(64, activation='swish')(x)\n\nx = tfkl.BatchNormalization(name='BatchNorm0')(x)\n\n# Add a Dense layer with 2 units and softmax activation as the classifier\noutputs = tfkl.Dense(2, activation='softmax')(x)\n\n# Create a Model connecting input and output\ntl_model = tfk.Model(inputs=inputs, outputs=outputs, name='model')\n\n# Define optimizer, loss, and metrics\n# AdamW is an Adam optimizer which applies weight_decay to network layers,\n# i.e it's another way to apply l2 regularization to the whole network\noptimizer = tfk.optimizers.AdamW(1e-4, weight_decay=5e-4)\nloss = tfk.losses.CategoricalCrossentropy()\nmetrics = ['accuracy']\n\n# Compile the model with Categorical Cross-Entropy loss and Adam optimizer\ntl_model.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n\n# Display model summary\ntl_model.summary()","metadata":{"execution":{"iopub.status.busy":"2023-11-09T16:40:21.737468Z","iopub.execute_input":"2023-11-09T16:40:21.737927Z","iopub.status.idle":"2023-11-09T16:40:24.829284Z","shell.execute_reply.started":"2023-11-09T16:40:21.737890Z","shell.execute_reply":"2023-11-09T16:40:24.828380Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"Model: \"model\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_4 (InputLayer)        [(None, 96, 96, 3)]       0         \n                                                                 \n preprocessing (Sequential)  (None, 96, 96, 3)         0         \n                                                                 \n efficientnetv2-m (Functiona  (None, 1280)             53150388  \n l)                                                              \n                                                                 \n dense_3 (Dense)             (None, 64)                81984     \n                                                                 \n BatchNorm0 (BatchNormalizat  (None, 64)               256       \n ion)                                                            \n                                                                 \n dense_4 (Dense)             (None, 2)                 130       \n                                                                 \n=================================================================\nTotal params: 53,232,758\nTrainable params: 82,242\nNon-trainable params: 53,150,516\n_________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":"# Train the model\ntl_history = tl_model.fit(\n    x = preprocess_input(X_train*255), # We need to apply the preprocessing thought for the MobileNetV2 network\n    y = y_train,\n    batch_size = 64,\n    epochs = 200,\n    validation_data = (preprocess_input(X_val*255), y_val), # We need to apply the preprocessing thought for the EfficientNetV2 network\n    callbacks = [tfk.callbacks.EarlyStopping(monitor='val_accuracy', mode='max', patience=20, restore_best_weights=True),\n                tfk.callbacks.ReduceLROnPlateau(monitor=\"val_accuracy\", factor=0.1, patience=20, min_lr=1e-5, mode='max')]\n).history","metadata":{"execution":{"iopub.status.busy":"2023-11-09T16:40:33.076469Z","iopub.execute_input":"2023-11-09T16:40:33.077144Z","iopub.status.idle":"2023-11-09T16:48:55.343707Z","shell.execute_reply.started":"2023-11-09T16:40:33.077108Z","shell.execute_reply":"2023-11-09T16:48:55.342650Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"Epoch 1/200\n62/62 [==============================] - 35s 193ms/step - loss: 0.7868 - accuracy: 0.5638 - val_loss: 0.6627 - val_accuracy: 0.6385 - lr: 1.0000e-04\nEpoch 2/200\n62/62 [==============================] - 7s 108ms/step - loss: 0.6016 - accuracy: 0.6781 - val_loss: 0.6316 - val_accuracy: 0.6808 - lr: 1.0000e-04\nEpoch 3/200\n62/62 [==============================] - 7s 108ms/step - loss: 0.5757 - accuracy: 0.7013 - val_loss: 0.5839 - val_accuracy: 0.7673 - lr: 1.0000e-04\nEpoch 4/200\n62/62 [==============================] - 7s 107ms/step - loss: 0.5498 - accuracy: 0.7202 - val_loss: 0.5559 - val_accuracy: 0.7904 - lr: 1.0000e-04\nEpoch 5/200\n62/62 [==============================] - 6s 101ms/step - loss: 0.5191 - accuracy: 0.7442 - val_loss: 0.5348 - val_accuracy: 0.7673 - lr: 1.0000e-04\nEpoch 6/200\n62/62 [==============================] - 6s 100ms/step - loss: 0.5226 - accuracy: 0.7409 - val_loss: 0.4967 - val_accuracy: 0.7750 - lr: 1.0000e-04\nEpoch 7/200\n62/62 [==============================] - 7s 105ms/step - loss: 0.5064 - accuracy: 0.7568 - val_loss: 0.4770 - val_accuracy: 0.7923 - lr: 1.0000e-04\nEpoch 8/200\n62/62 [==============================] - 7s 106ms/step - loss: 0.4904 - accuracy: 0.7664 - val_loss: 0.4497 - val_accuracy: 0.8019 - lr: 1.0000e-04\nEpoch 9/200\n62/62 [==============================] - 6s 99ms/step - loss: 0.4900 - accuracy: 0.7593 - val_loss: 0.4462 - val_accuracy: 0.7923 - lr: 1.0000e-04\nEpoch 10/200\n62/62 [==============================] - 6s 99ms/step - loss: 0.4888 - accuracy: 0.7634 - val_loss: 0.4503 - val_accuracy: 0.7962 - lr: 1.0000e-04\nEpoch 11/200\n62/62 [==============================] - 6s 99ms/step - loss: 0.4821 - accuracy: 0.7634 - val_loss: 0.4430 - val_accuracy: 0.7962 - lr: 1.0000e-04\nEpoch 12/200\n62/62 [==============================] - 6s 99ms/step - loss: 0.4826 - accuracy: 0.7689 - val_loss: 0.4406 - val_accuracy: 0.8019 - lr: 1.0000e-04\nEpoch 13/200\n62/62 [==============================] - 6s 100ms/step - loss: 0.4844 - accuracy: 0.7654 - val_loss: 0.4313 - val_accuracy: 0.8000 - lr: 1.0000e-04\nEpoch 14/200\n62/62 [==============================] - 6s 99ms/step - loss: 0.4769 - accuracy: 0.7674 - val_loss: 0.4454 - val_accuracy: 0.7962 - lr: 1.0000e-04\nEpoch 15/200\n62/62 [==============================] - 6s 99ms/step - loss: 0.4728 - accuracy: 0.7745 - val_loss: 0.4527 - val_accuracy: 0.7942 - lr: 1.0000e-04\nEpoch 16/200\n62/62 [==============================] - 6s 100ms/step - loss: 0.4763 - accuracy: 0.7717 - val_loss: 0.4579 - val_accuracy: 0.7885 - lr: 1.0000e-04\nEpoch 17/200\n62/62 [==============================] - 7s 106ms/step - loss: 0.4657 - accuracy: 0.7856 - val_loss: 0.4304 - val_accuracy: 0.8058 - lr: 1.0000e-04\nEpoch 18/200\n62/62 [==============================] - 6s 100ms/step - loss: 0.4697 - accuracy: 0.7735 - val_loss: 0.4356 - val_accuracy: 0.7962 - lr: 1.0000e-04\nEpoch 19/200\n62/62 [==============================] - 6s 105ms/step - loss: 0.4689 - accuracy: 0.7730 - val_loss: 0.4260 - val_accuracy: 0.8115 - lr: 1.0000e-04\nEpoch 20/200\n62/62 [==============================] - 6s 99ms/step - loss: 0.4822 - accuracy: 0.7687 - val_loss: 0.4266 - val_accuracy: 0.8038 - lr: 1.0000e-04\nEpoch 21/200\n62/62 [==============================] - 6s 99ms/step - loss: 0.4599 - accuracy: 0.7823 - val_loss: 0.4200 - val_accuracy: 0.8077 - lr: 1.0000e-04\nEpoch 22/200\n62/62 [==============================] - 6s 99ms/step - loss: 0.4645 - accuracy: 0.7730 - val_loss: 0.4283 - val_accuracy: 0.8038 - lr: 1.0000e-04\nEpoch 23/200\n62/62 [==============================] - 6s 100ms/step - loss: 0.4590 - accuracy: 0.7760 - val_loss: 0.4759 - val_accuracy: 0.7615 - lr: 1.0000e-04\nEpoch 24/200\n62/62 [==============================] - 6s 99ms/step - loss: 0.4546 - accuracy: 0.7853 - val_loss: 0.4257 - val_accuracy: 0.7981 - lr: 1.0000e-04\nEpoch 25/200\n62/62 [==============================] - 6s 98ms/step - loss: 0.4542 - accuracy: 0.7828 - val_loss: 0.4195 - val_accuracy: 0.8077 - lr: 1.0000e-04\nEpoch 26/200\n62/62 [==============================] - 6s 99ms/step - loss: 0.4550 - accuracy: 0.7785 - val_loss: 0.4153 - val_accuracy: 0.8077 - lr: 1.0000e-04\nEpoch 27/200\n62/62 [==============================] - 6s 99ms/step - loss: 0.4561 - accuracy: 0.7843 - val_loss: 0.4194 - val_accuracy: 0.8077 - lr: 1.0000e-04\nEpoch 28/200\n62/62 [==============================] - 6s 104ms/step - loss: 0.4538 - accuracy: 0.7775 - val_loss: 0.4166 - val_accuracy: 0.8173 - lr: 1.0000e-04\nEpoch 29/200\n62/62 [==============================] - 6s 99ms/step - loss: 0.4570 - accuracy: 0.7828 - val_loss: 0.4457 - val_accuracy: 0.7942 - lr: 1.0000e-04\nEpoch 30/200\n62/62 [==============================] - 6s 98ms/step - loss: 0.4475 - accuracy: 0.7929 - val_loss: 0.4138 - val_accuracy: 0.8135 - lr: 1.0000e-04\nEpoch 31/200\n62/62 [==============================] - 6s 104ms/step - loss: 0.4618 - accuracy: 0.7861 - val_loss: 0.4033 - val_accuracy: 0.8231 - lr: 1.0000e-04\nEpoch 32/200\n62/62 [==============================] - 6s 99ms/step - loss: 0.4527 - accuracy: 0.7846 - val_loss: 0.4549 - val_accuracy: 0.7827 - lr: 1.0000e-04\nEpoch 33/200\n62/62 [==============================] - 6s 105ms/step - loss: 0.4493 - accuracy: 0.7876 - val_loss: 0.4033 - val_accuracy: 0.8250 - lr: 1.0000e-04\nEpoch 34/200\n62/62 [==============================] - 6s 99ms/step - loss: 0.4650 - accuracy: 0.7841 - val_loss: 0.4550 - val_accuracy: 0.7750 - lr: 1.0000e-04\nEpoch 35/200\n62/62 [==============================] - 6s 99ms/step - loss: 0.4663 - accuracy: 0.7838 - val_loss: 0.4150 - val_accuracy: 0.8173 - lr: 1.0000e-04\nEpoch 36/200\n62/62 [==============================] - 6s 104ms/step - loss: 0.4472 - accuracy: 0.7926 - val_loss: 0.4013 - val_accuracy: 0.8288 - lr: 1.0000e-04\nEpoch 37/200\n62/62 [==============================] - 6s 99ms/step - loss: 0.4453 - accuracy: 0.7856 - val_loss: 0.4037 - val_accuracy: 0.8173 - lr: 1.0000e-04\nEpoch 38/200\n62/62 [==============================] - 6s 99ms/step - loss: 0.4418 - accuracy: 0.7906 - val_loss: 0.4013 - val_accuracy: 0.8250 - lr: 1.0000e-04\nEpoch 39/200\n62/62 [==============================] - 6s 99ms/step - loss: 0.4359 - accuracy: 0.7934 - val_loss: 0.4056 - val_accuracy: 0.8212 - lr: 1.0000e-04\nEpoch 40/200\n62/62 [==============================] - 6s 99ms/step - loss: 0.4545 - accuracy: 0.7843 - val_loss: 0.4084 - val_accuracy: 0.8154 - lr: 1.0000e-04\nEpoch 41/200\n62/62 [==============================] - 6s 98ms/step - loss: 0.4358 - accuracy: 0.7916 - val_loss: 0.4172 - val_accuracy: 0.8173 - lr: 1.0000e-04\nEpoch 42/200\n62/62 [==============================] - 6s 99ms/step - loss: 0.4554 - accuracy: 0.7813 - val_loss: 0.4122 - val_accuracy: 0.8173 - lr: 1.0000e-04\nEpoch 43/200\n62/62 [==============================] - 7s 106ms/step - loss: 0.4496 - accuracy: 0.7914 - val_loss: 0.4099 - val_accuracy: 0.8173 - lr: 1.0000e-04\nEpoch 44/200\n62/62 [==============================] - 6s 99ms/step - loss: 0.4451 - accuracy: 0.7899 - val_loss: 0.4077 - val_accuracy: 0.8269 - lr: 1.0000e-04\nEpoch 45/200\n62/62 [==============================] - 6s 99ms/step - loss: 0.4454 - accuracy: 0.7931 - val_loss: 0.4120 - val_accuracy: 0.8115 - lr: 1.0000e-04\nEpoch 46/200\n62/62 [==============================] - 6s 98ms/step - loss: 0.4431 - accuracy: 0.7954 - val_loss: 0.4092 - val_accuracy: 0.8173 - lr: 1.0000e-04\nEpoch 47/200\n62/62 [==============================] - 6s 105ms/step - loss: 0.4478 - accuracy: 0.7954 - val_loss: 0.3979 - val_accuracy: 0.8308 - lr: 1.0000e-04\nEpoch 48/200\n62/62 [==============================] - 6s 99ms/step - loss: 0.4450 - accuracy: 0.7878 - val_loss: 0.4268 - val_accuracy: 0.8250 - lr: 1.0000e-04\nEpoch 49/200\n62/62 [==============================] - 6s 100ms/step - loss: 0.4459 - accuracy: 0.7904 - val_loss: 0.4190 - val_accuracy: 0.8173 - lr: 1.0000e-04\nEpoch 50/200\n62/62 [==============================] - 6s 99ms/step - loss: 0.4431 - accuracy: 0.7926 - val_loss: 0.4134 - val_accuracy: 0.8096 - lr: 1.0000e-04\nEpoch 51/200\n62/62 [==============================] - 6s 99ms/step - loss: 0.4246 - accuracy: 0.8022 - val_loss: 0.4163 - val_accuracy: 0.8115 - lr: 1.0000e-04\nEpoch 52/200\n62/62 [==============================] - 6s 99ms/step - loss: 0.4421 - accuracy: 0.7919 - val_loss: 0.3951 - val_accuracy: 0.8288 - lr: 1.0000e-04\nEpoch 53/200\n62/62 [==============================] - 6s 99ms/step - loss: 0.4292 - accuracy: 0.7962 - val_loss: 0.4349 - val_accuracy: 0.8058 - lr: 1.0000e-04\nEpoch 54/200\n62/62 [==============================] - 6s 100ms/step - loss: 0.4546 - accuracy: 0.7871 - val_loss: 0.3965 - val_accuracy: 0.8250 - lr: 1.0000e-04\nEpoch 55/200\n62/62 [==============================] - 6s 99ms/step - loss: 0.4334 - accuracy: 0.8017 - val_loss: 0.4153 - val_accuracy: 0.8096 - lr: 1.0000e-04\nEpoch 56/200\n62/62 [==============================] - 6s 105ms/step - loss: 0.4302 - accuracy: 0.7982 - val_loss: 0.3866 - val_accuracy: 0.8423 - lr: 1.0000e-04\nEpoch 57/200\n62/62 [==============================] - 6s 99ms/step - loss: 0.4480 - accuracy: 0.7919 - val_loss: 0.4098 - val_accuracy: 0.8154 - lr: 1.0000e-04\nEpoch 58/200\n62/62 [==============================] - 6s 99ms/step - loss: 0.4261 - accuracy: 0.8010 - val_loss: 0.3943 - val_accuracy: 0.8231 - lr: 1.0000e-04\nEpoch 59/200\n62/62 [==============================] - 6s 100ms/step - loss: 0.4312 - accuracy: 0.7974 - val_loss: 0.3922 - val_accuracy: 0.8308 - lr: 1.0000e-04\nEpoch 60/200\n62/62 [==============================] - 6s 99ms/step - loss: 0.4318 - accuracy: 0.7994 - val_loss: 0.3944 - val_accuracy: 0.8250 - lr: 1.0000e-04\nEpoch 61/200\n62/62 [==============================] - 6s 99ms/step - loss: 0.4370 - accuracy: 0.7936 - val_loss: 0.4171 - val_accuracy: 0.8096 - lr: 1.0000e-04\nEpoch 62/200\n62/62 [==============================] - 6s 99ms/step - loss: 0.4414 - accuracy: 0.7941 - val_loss: 0.4261 - val_accuracy: 0.8019 - lr: 1.0000e-04\nEpoch 63/200\n62/62 [==============================] - 6s 99ms/step - loss: 0.4430 - accuracy: 0.7921 - val_loss: 0.4057 - val_accuracy: 0.8135 - lr: 1.0000e-04\nEpoch 64/200\n62/62 [==============================] - 6s 100ms/step - loss: 0.4333 - accuracy: 0.7964 - val_loss: 0.3913 - val_accuracy: 0.8346 - lr: 1.0000e-04\nEpoch 65/200\n62/62 [==============================] - 6s 99ms/step - loss: 0.4307 - accuracy: 0.8027 - val_loss: 0.4012 - val_accuracy: 0.8288 - lr: 1.0000e-04\nEpoch 66/200\n62/62 [==============================] - 6s 99ms/step - loss: 0.4316 - accuracy: 0.7984 - val_loss: 0.4086 - val_accuracy: 0.8173 - lr: 1.0000e-04\nEpoch 67/200\n62/62 [==============================] - 6s 99ms/step - loss: 0.4298 - accuracy: 0.7984 - val_loss: 0.3939 - val_accuracy: 0.8212 - lr: 1.0000e-04\nEpoch 68/200\n62/62 [==============================] - 6s 100ms/step - loss: 0.4403 - accuracy: 0.7987 - val_loss: 0.4289 - val_accuracy: 0.8077 - lr: 1.0000e-04\nEpoch 69/200\n62/62 [==============================] - 6s 100ms/step - loss: 0.4328 - accuracy: 0.8017 - val_loss: 0.4031 - val_accuracy: 0.8250 - lr: 1.0000e-04\nEpoch 70/200\n62/62 [==============================] - 6s 99ms/step - loss: 0.4309 - accuracy: 0.8012 - val_loss: 0.3938 - val_accuracy: 0.8288 - lr: 1.0000e-04\nEpoch 71/200\n62/62 [==============================] - 6s 100ms/step - loss: 0.4300 - accuracy: 0.7959 - val_loss: 0.4205 - val_accuracy: 0.8173 - lr: 1.0000e-04\nEpoch 72/200\n62/62 [==============================] - 6s 99ms/step - loss: 0.4210 - accuracy: 0.8080 - val_loss: 0.3911 - val_accuracy: 0.8212 - lr: 1.0000e-04\nEpoch 73/200\n62/62 [==============================] - 6s 100ms/step - loss: 0.4212 - accuracy: 0.8075 - val_loss: 0.3893 - val_accuracy: 0.8308 - lr: 1.0000e-04\nEpoch 74/200\n62/62 [==============================] - 6s 100ms/step - loss: 0.4381 - accuracy: 0.7989 - val_loss: 0.3998 - val_accuracy: 0.8385 - lr: 1.0000e-04\nEpoch 75/200\n62/62 [==============================] - 6s 99ms/step - loss: 0.4257 - accuracy: 0.8012 - val_loss: 0.3901 - val_accuracy: 0.8192 - lr: 1.0000e-04\nEpoch 76/200\n62/62 [==============================] - 7s 107ms/step - loss: 0.4339 - accuracy: 0.7979 - val_loss: 0.3819 - val_accuracy: 0.8346 - lr: 1.0000e-04\n","output_type":"stream"}]},{"cell_type":"code","source":"# Evaluate the model on the test set\ntest_accuracy = tl_model.evaluate(preprocess_input(X_test*255),y_test,verbose=0)[-1]\nprint('Test set accuracy %.4f' % test_accuracy)\nfrom sklearn.metrics import precision_score, recall_score\ny_pred = tl_model.predict(preprocess_input(X_test*255))\ny_pred = tf.argmax(y_pred, axis=-1)\ny_test_true = np.argmax(y_test, axis=-1)\n# Calculate precision and recall\nprecision = precision_score(y_test_true, y_pred)\nrecall = recall_score(y_test_true, y_pred)\n\n# Print the precision and recall\nprint(\"Precision:\", precision)\nprint(\"Recall:\", recall)","metadata":{"execution":{"iopub.status.busy":"2023-11-09T16:49:38.558496Z","iopub.execute_input":"2023-11-09T16:49:38.558872Z","iopub.status.idle":"2023-11-09T16:49:45.278340Z","shell.execute_reply.started":"2023-11-09T16:49:38.558841Z","shell.execute_reply":"2023-11-09T16:49:45.277447Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"Test set accuracy 0.8288\n17/17 [==============================] - 5s 56ms/step\nPrecision: 0.8114285714285714\nRecall: 0.7171717171717171\n","output_type":"stream"}]},{"cell_type":"code","source":"# Save the best model\ntl_model.save('EfficientNet-AUG-FC-BATCHNORM-TL')\ndel tl_model","metadata":{"execution":{"iopub.status.busy":"2023-11-09T17:35:31.939237Z","iopub.execute_input":"2023-11-09T17:35:31.939513Z","iopub.status.idle":"2023-11-09T17:35:32.710079Z","shell.execute_reply.started":"2023-11-09T17:35:31.939486Z","shell.execute_reply":"2023-11-09T17:35:32.708706Z"},"trusted":true},"execution_count":8,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[8], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Save the best model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtl_model\u001b[49m\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEfficientNet-AUG-FC-BATCHNORM-TL\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m tl_model\n","\u001b[0;31mNameError\u001b[0m: name 'tl_model' is not defined"],"ename":"NameError","evalue":"name 'tl_model' is not defined","output_type":"error"}]},{"cell_type":"code","source":"# Re-load the model after transfer learning\nft_model = tfk.models.load_model('EfficientNet-AUG-FC-BATCHNORM-TL')\nft_model.summary()","metadata":{"execution":{"iopub.status.busy":"2023-11-09T17:36:11.581888Z","iopub.execute_input":"2023-11-09T17:36:11.582541Z","iopub.status.idle":"2023-11-09T17:37:02.444335Z","shell.execute_reply.started":"2023-11-09T17:36:11.582510Z","shell.execute_reply":"2023-11-09T17:37:02.443419Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Model: \"model\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_4 (InputLayer)        [(None, 96, 96, 3)]       0         \n                                                                 \n preprocessing (Sequential)  (None, 96, 96, 3)         0         \n                                                                 \n efficientnetv2-m (Functiona  (None, 1280)             53150388  \n l)                                                              \n                                                                 \n dense_3 (Dense)             (None, 64)                81984     \n                                                                 \n BatchNorm0 (BatchNormalizat  (None, 64)               256       \n ion)                                                            \n                                                                 \n dense_4 (Dense)             (None, 2)                 130       \n                                                                 \n=================================================================\nTotal params: 53,232,758\nTrainable params: 82,242\nNon-trainable params: 53,150,516\n_________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":"# Set all network layers as trainable\nft_model.get_layer('efficientnetv2-m').trainable = True","metadata":{"execution":{"iopub.status.busy":"2023-11-09T17:37:48.902477Z","iopub.execute_input":"2023-11-09T17:37:48.902872Z","iopub.status.idle":"2023-11-09T17:37:48.945305Z","shell.execute_reply.started":"2023-11-09T17:37:48.902840Z","shell.execute_reply":"2023-11-09T17:37:48.944488Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"N = 664\nfor i, layer in enumerate(ft_model.get_layer('efficientnetv2-m').layers[:N]):\n  layer.trainable=False\nfor i, layer in enumerate(ft_model.get_layer('efficientnetv2-m').layers):\n   print(i, layer.name, layer.trainable)\nft_model.summary()","metadata":{"execution":{"iopub.status.busy":"2023-11-09T17:38:28.175401Z","iopub.execute_input":"2023-11-09T17:38:28.175796Z","iopub.status.idle":"2023-11-09T17:38:28.311840Z","shell.execute_reply.started":"2023-11-09T17:38:28.175765Z","shell.execute_reply":"2023-11-09T17:38:28.310900Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"0 input_1 False\n1 rescaling False\n2 stem_conv False\n3 stem_bn False\n4 stem_activation False\n5 block1a_project_conv False\n6 block1a_project_bn False\n7 block1a_project_activation False\n8 block1a_add False\n9 block1b_project_conv False\n10 block1b_project_bn False\n11 block1b_project_activation False\n12 block1b_drop False\n13 block1b_add False\n14 block1c_project_conv False\n15 block1c_project_bn False\n16 block1c_project_activation False\n17 block1c_drop False\n18 block1c_add False\n19 block2a_expand_conv False\n20 block2a_expand_bn False\n21 block2a_expand_activation False\n22 block2a_project_conv False\n23 block2a_project_bn False\n24 block2b_expand_conv False\n25 block2b_expand_bn False\n26 block2b_expand_activation False\n27 block2b_project_conv False\n28 block2b_project_bn False\n29 block2b_drop False\n30 block2b_add False\n31 block2c_expand_conv False\n32 block2c_expand_bn False\n33 block2c_expand_activation False\n34 block2c_project_conv False\n35 block2c_project_bn False\n36 block2c_drop False\n37 block2c_add False\n38 block2d_expand_conv False\n39 block2d_expand_bn False\n40 block2d_expand_activation False\n41 block2d_project_conv False\n42 block2d_project_bn False\n43 block2d_drop False\n44 block2d_add False\n45 block2e_expand_conv False\n46 block2e_expand_bn False\n47 block2e_expand_activation False\n48 block2e_project_conv False\n49 block2e_project_bn False\n50 block2e_drop False\n51 block2e_add False\n52 block3a_expand_conv False\n53 block3a_expand_bn False\n54 block3a_expand_activation False\n55 block3a_project_conv False\n56 block3a_project_bn False\n57 block3b_expand_conv False\n58 block3b_expand_bn False\n59 block3b_expand_activation False\n60 block3b_project_conv False\n61 block3b_project_bn False\n62 block3b_drop False\n63 block3b_add False\n64 block3c_expand_conv False\n65 block3c_expand_bn False\n66 block3c_expand_activation False\n67 block3c_project_conv False\n68 block3c_project_bn False\n69 block3c_drop False\n70 block3c_add False\n71 block3d_expand_conv False\n72 block3d_expand_bn False\n73 block3d_expand_activation False\n74 block3d_project_conv False\n75 block3d_project_bn False\n76 block3d_drop False\n77 block3d_add False\n78 block3e_expand_conv False\n79 block3e_expand_bn False\n80 block3e_expand_activation False\n81 block3e_project_conv False\n82 block3e_project_bn False\n83 block3e_drop False\n84 block3e_add False\n85 block4a_expand_conv False\n86 block4a_expand_bn False\n87 block4a_expand_activation False\n88 block4a_dwconv2 False\n89 block4a_bn False\n90 block4a_activation False\n91 block4a_se_squeeze False\n92 block4a_se_reshape False\n93 block4a_se_reduce False\n94 block4a_se_expand False\n95 block4a_se_excite False\n96 block4a_project_conv False\n97 block4a_project_bn False\n98 block4b_expand_conv False\n99 block4b_expand_bn False\n100 block4b_expand_activation False\n101 block4b_dwconv2 False\n102 block4b_bn False\n103 block4b_activation False\n104 block4b_se_squeeze False\n105 block4b_se_reshape False\n106 block4b_se_reduce False\n107 block4b_se_expand False\n108 block4b_se_excite False\n109 block4b_project_conv False\n110 block4b_project_bn False\n111 block4b_drop False\n112 block4b_add False\n113 block4c_expand_conv False\n114 block4c_expand_bn False\n115 block4c_expand_activation False\n116 block4c_dwconv2 False\n117 block4c_bn False\n118 block4c_activation False\n119 block4c_se_squeeze False\n120 block4c_se_reshape False\n121 block4c_se_reduce False\n122 block4c_se_expand False\n123 block4c_se_excite False\n124 block4c_project_conv False\n125 block4c_project_bn False\n126 block4c_drop False\n127 block4c_add False\n128 block4d_expand_conv False\n129 block4d_expand_bn False\n130 block4d_expand_activation False\n131 block4d_dwconv2 False\n132 block4d_bn False\n133 block4d_activation False\n134 block4d_se_squeeze False\n135 block4d_se_reshape False\n136 block4d_se_reduce False\n137 block4d_se_expand False\n138 block4d_se_excite False\n139 block4d_project_conv False\n140 block4d_project_bn False\n141 block4d_drop False\n142 block4d_add False\n143 block4e_expand_conv False\n144 block4e_expand_bn False\n145 block4e_expand_activation False\n146 block4e_dwconv2 False\n147 block4e_bn False\n148 block4e_activation False\n149 block4e_se_squeeze False\n150 block4e_se_reshape False\n151 block4e_se_reduce False\n152 block4e_se_expand False\n153 block4e_se_excite False\n154 block4e_project_conv False\n155 block4e_project_bn False\n156 block4e_drop False\n157 block4e_add False\n158 block4f_expand_conv False\n159 block4f_expand_bn False\n160 block4f_expand_activation False\n161 block4f_dwconv2 False\n162 block4f_bn False\n163 block4f_activation False\n164 block4f_se_squeeze False\n165 block4f_se_reshape False\n166 block4f_se_reduce False\n167 block4f_se_expand False\n168 block4f_se_excite False\n169 block4f_project_conv False\n170 block4f_project_bn False\n171 block4f_drop False\n172 block4f_add False\n173 block4g_expand_conv False\n174 block4g_expand_bn False\n175 block4g_expand_activation False\n176 block4g_dwconv2 False\n177 block4g_bn False\n178 block4g_activation False\n179 block4g_se_squeeze False\n180 block4g_se_reshape False\n181 block4g_se_reduce False\n182 block4g_se_expand False\n183 block4g_se_excite False\n184 block4g_project_conv False\n185 block4g_project_bn False\n186 block4g_drop False\n187 block4g_add False\n188 block5a_expand_conv False\n189 block5a_expand_bn False\n190 block5a_expand_activation False\n191 block5a_dwconv2 False\n192 block5a_bn False\n193 block5a_activation False\n194 block5a_se_squeeze False\n195 block5a_se_reshape False\n196 block5a_se_reduce False\n197 block5a_se_expand False\n198 block5a_se_excite False\n199 block5a_project_conv False\n200 block5a_project_bn False\n201 block5b_expand_conv False\n202 block5b_expand_bn False\n203 block5b_expand_activation False\n204 block5b_dwconv2 False\n205 block5b_bn False\n206 block5b_activation False\n207 block5b_se_squeeze False\n208 block5b_se_reshape False\n209 block5b_se_reduce False\n210 block5b_se_expand False\n211 block5b_se_excite False\n212 block5b_project_conv False\n213 block5b_project_bn False\n214 block5b_drop False\n215 block5b_add False\n216 block5c_expand_conv False\n217 block5c_expand_bn False\n218 block5c_expand_activation False\n219 block5c_dwconv2 False\n220 block5c_bn False\n221 block5c_activation False\n222 block5c_se_squeeze False\n223 block5c_se_reshape False\n224 block5c_se_reduce False\n225 block5c_se_expand False\n226 block5c_se_excite False\n227 block5c_project_conv False\n228 block5c_project_bn False\n229 block5c_drop False\n230 block5c_add False\n231 block5d_expand_conv False\n232 block5d_expand_bn False\n233 block5d_expand_activation False\n234 block5d_dwconv2 False\n235 block5d_bn False\n236 block5d_activation False\n237 block5d_se_squeeze False\n238 block5d_se_reshape False\n239 block5d_se_reduce False\n240 block5d_se_expand False\n241 block5d_se_excite False\n242 block5d_project_conv False\n243 block5d_project_bn False\n244 block5d_drop False\n245 block5d_add False\n246 block5e_expand_conv False\n247 block5e_expand_bn False\n248 block5e_expand_activation False\n249 block5e_dwconv2 False\n250 block5e_bn False\n251 block5e_activation False\n252 block5e_se_squeeze False\n253 block5e_se_reshape False\n254 block5e_se_reduce False\n255 block5e_se_expand False\n256 block5e_se_excite False\n257 block5e_project_conv False\n258 block5e_project_bn False\n259 block5e_drop False\n260 block5e_add False\n261 block5f_expand_conv False\n262 block5f_expand_bn False\n263 block5f_expand_activation False\n264 block5f_dwconv2 False\n265 block5f_bn False\n266 block5f_activation False\n267 block5f_se_squeeze False\n268 block5f_se_reshape False\n269 block5f_se_reduce False\n270 block5f_se_expand False\n271 block5f_se_excite False\n272 block5f_project_conv False\n273 block5f_project_bn False\n274 block5f_drop False\n275 block5f_add False\n276 block5g_expand_conv False\n277 block5g_expand_bn False\n278 block5g_expand_activation False\n279 block5g_dwconv2 False\n280 block5g_bn False\n281 block5g_activation False\n282 block5g_se_squeeze False\n283 block5g_se_reshape False\n284 block5g_se_reduce False\n285 block5g_se_expand False\n286 block5g_se_excite False\n287 block5g_project_conv False\n288 block5g_project_bn False\n289 block5g_drop False\n290 block5g_add False\n291 block5h_expand_conv False\n292 block5h_expand_bn False\n293 block5h_expand_activation False\n294 block5h_dwconv2 False\n295 block5h_bn False\n296 block5h_activation False\n297 block5h_se_squeeze False\n298 block5h_se_reshape False\n299 block5h_se_reduce False\n300 block5h_se_expand False\n301 block5h_se_excite False\n302 block5h_project_conv False\n303 block5h_project_bn False\n304 block5h_drop False\n305 block5h_add False\n306 block5i_expand_conv False\n307 block5i_expand_bn False\n308 block5i_expand_activation False\n309 block5i_dwconv2 False\n310 block5i_bn False\n311 block5i_activation False\n312 block5i_se_squeeze False\n313 block5i_se_reshape False\n314 block5i_se_reduce False\n315 block5i_se_expand False\n316 block5i_se_excite False\n317 block5i_project_conv False\n318 block5i_project_bn False\n319 block5i_drop False\n320 block5i_add False\n321 block5j_expand_conv False\n322 block5j_expand_bn False\n323 block5j_expand_activation False\n324 block5j_dwconv2 False\n325 block5j_bn False\n326 block5j_activation False\n327 block5j_se_squeeze False\n328 block5j_se_reshape False\n329 block5j_se_reduce False\n330 block5j_se_expand False\n331 block5j_se_excite False\n332 block5j_project_conv False\n333 block5j_project_bn False\n334 block5j_drop False\n335 block5j_add False\n336 block5k_expand_conv False\n337 block5k_expand_bn False\n338 block5k_expand_activation False\n339 block5k_dwconv2 False\n340 block5k_bn False\n341 block5k_activation False\n342 block5k_se_squeeze False\n343 block5k_se_reshape False\n344 block5k_se_reduce False\n345 block5k_se_expand False\n346 block5k_se_excite False\n347 block5k_project_conv False\n348 block5k_project_bn False\n349 block5k_drop False\n350 block5k_add False\n351 block5l_expand_conv False\n352 block5l_expand_bn False\n353 block5l_expand_activation False\n354 block5l_dwconv2 False\n355 block5l_bn False\n356 block5l_activation False\n357 block5l_se_squeeze False\n358 block5l_se_reshape False\n359 block5l_se_reduce False\n360 block5l_se_expand False\n361 block5l_se_excite False\n362 block5l_project_conv False\n363 block5l_project_bn False\n364 block5l_drop False\n365 block5l_add False\n366 block5m_expand_conv False\n367 block5m_expand_bn False\n368 block5m_expand_activation False\n369 block5m_dwconv2 False\n370 block5m_bn False\n371 block5m_activation False\n372 block5m_se_squeeze False\n373 block5m_se_reshape False\n374 block5m_se_reduce False\n375 block5m_se_expand False\n376 block5m_se_excite False\n377 block5m_project_conv False\n378 block5m_project_bn False\n379 block5m_drop False\n380 block5m_add False\n381 block5n_expand_conv False\n382 block5n_expand_bn False\n383 block5n_expand_activation False\n384 block5n_dwconv2 False\n385 block5n_bn False\n386 block5n_activation False\n387 block5n_se_squeeze False\n388 block5n_se_reshape False\n389 block5n_se_reduce False\n390 block5n_se_expand False\n391 block5n_se_excite False\n392 block5n_project_conv False\n393 block5n_project_bn False\n394 block5n_drop False\n395 block5n_add False\n396 block6a_expand_conv False\n397 block6a_expand_bn False\n398 block6a_expand_activation False\n399 block6a_dwconv2 False\n400 block6a_bn False\n401 block6a_activation False\n402 block6a_se_squeeze False\n403 block6a_se_reshape False\n404 block6a_se_reduce False\n405 block6a_se_expand False\n406 block6a_se_excite False\n407 block6a_project_conv False\n408 block6a_project_bn False\n409 block6b_expand_conv False\n410 block6b_expand_bn False\n411 block6b_expand_activation False\n412 block6b_dwconv2 False\n413 block6b_bn False\n414 block6b_activation False\n415 block6b_se_squeeze False\n416 block6b_se_reshape False\n417 block6b_se_reduce False\n418 block6b_se_expand False\n419 block6b_se_excite False\n420 block6b_project_conv False\n421 block6b_project_bn False\n422 block6b_drop False\n423 block6b_add False\n424 block6c_expand_conv False\n425 block6c_expand_bn False\n426 block6c_expand_activation False\n427 block6c_dwconv2 False\n428 block6c_bn False\n429 block6c_activation False\n430 block6c_se_squeeze False\n431 block6c_se_reshape False\n432 block6c_se_reduce False\n433 block6c_se_expand False\n434 block6c_se_excite False\n435 block6c_project_conv False\n436 block6c_project_bn False\n437 block6c_drop False\n438 block6c_add False\n439 block6d_expand_conv False\n440 block6d_expand_bn False\n441 block6d_expand_activation False\n442 block6d_dwconv2 False\n443 block6d_bn False\n444 block6d_activation False\n445 block6d_se_squeeze False\n446 block6d_se_reshape False\n447 block6d_se_reduce False\n448 block6d_se_expand False\n449 block6d_se_excite False\n450 block6d_project_conv False\n451 block6d_project_bn False\n452 block6d_drop False\n453 block6d_add False\n454 block6e_expand_conv False\n455 block6e_expand_bn False\n456 block6e_expand_activation False\n457 block6e_dwconv2 False\n458 block6e_bn False\n459 block6e_activation False\n460 block6e_se_squeeze False\n461 block6e_se_reshape False\n462 block6e_se_reduce False\n463 block6e_se_expand False\n464 block6e_se_excite False\n465 block6e_project_conv False\n466 block6e_project_bn False\n467 block6e_drop False\n468 block6e_add False\n469 block6f_expand_conv False\n470 block6f_expand_bn False\n471 block6f_expand_activation False\n472 block6f_dwconv2 False\n473 block6f_bn False\n474 block6f_activation False\n475 block6f_se_squeeze False\n476 block6f_se_reshape False\n477 block6f_se_reduce False\n478 block6f_se_expand False\n479 block6f_se_excite False\n480 block6f_project_conv False\n481 block6f_project_bn False\n482 block6f_drop False\n483 block6f_add False\n484 block6g_expand_conv False\n485 block6g_expand_bn False\n486 block6g_expand_activation False\n487 block6g_dwconv2 False\n488 block6g_bn False\n489 block6g_activation False\n490 block6g_se_squeeze False\n491 block6g_se_reshape False\n492 block6g_se_reduce False\n493 block6g_se_expand False\n494 block6g_se_excite False\n495 block6g_project_conv False\n496 block6g_project_bn False\n497 block6g_drop False\n498 block6g_add False\n499 block6h_expand_conv False\n500 block6h_expand_bn False\n501 block6h_expand_activation False\n502 block6h_dwconv2 False\n503 block6h_bn False\n504 block6h_activation False\n505 block6h_se_squeeze False\n506 block6h_se_reshape False\n507 block6h_se_reduce False\n508 block6h_se_expand False\n509 block6h_se_excite False\n510 block6h_project_conv False\n511 block6h_project_bn False\n512 block6h_drop False\n513 block6h_add False\n514 block6i_expand_conv False\n515 block6i_expand_bn False\n516 block6i_expand_activation False\n517 block6i_dwconv2 False\n518 block6i_bn False\n519 block6i_activation False\n520 block6i_se_squeeze False\n521 block6i_se_reshape False\n522 block6i_se_reduce False\n523 block6i_se_expand False\n524 block6i_se_excite False\n525 block6i_project_conv False\n526 block6i_project_bn False\n527 block6i_drop False\n528 block6i_add False\n529 block6j_expand_conv False\n530 block6j_expand_bn False\n531 block6j_expand_activation False\n532 block6j_dwconv2 False\n533 block6j_bn False\n534 block6j_activation False\n535 block6j_se_squeeze False\n536 block6j_se_reshape False\n537 block6j_se_reduce False\n538 block6j_se_expand False\n539 block6j_se_excite False\n540 block6j_project_conv False\n541 block6j_project_bn False\n542 block6j_drop False\n543 block6j_add False\n544 block6k_expand_conv False\n545 block6k_expand_bn False\n546 block6k_expand_activation False\n547 block6k_dwconv2 False\n548 block6k_bn False\n549 block6k_activation False\n550 block6k_se_squeeze False\n551 block6k_se_reshape False\n552 block6k_se_reduce False\n553 block6k_se_expand False\n554 block6k_se_excite False\n555 block6k_project_conv False\n556 block6k_project_bn False\n557 block6k_drop False\n558 block6k_add False\n559 block6l_expand_conv False\n560 block6l_expand_bn False\n561 block6l_expand_activation False\n562 block6l_dwconv2 False\n563 block6l_bn False\n564 block6l_activation False\n565 block6l_se_squeeze False\n566 block6l_se_reshape False\n567 block6l_se_reduce False\n568 block6l_se_expand False\n569 block6l_se_excite False\n570 block6l_project_conv False\n571 block6l_project_bn False\n572 block6l_drop False\n573 block6l_add False\n574 block6m_expand_conv False\n575 block6m_expand_bn False\n576 block6m_expand_activation False\n577 block6m_dwconv2 False\n578 block6m_bn False\n579 block6m_activation False\n580 block6m_se_squeeze False\n581 block6m_se_reshape False\n582 block6m_se_reduce False\n583 block6m_se_expand False\n584 block6m_se_excite False\n585 block6m_project_conv False\n586 block6m_project_bn False\n587 block6m_drop False\n588 block6m_add False\n589 block6n_expand_conv False\n590 block6n_expand_bn False\n591 block6n_expand_activation False\n592 block6n_dwconv2 False\n593 block6n_bn False\n594 block6n_activation False\n595 block6n_se_squeeze False\n596 block6n_se_reshape False\n597 block6n_se_reduce False\n598 block6n_se_expand False\n599 block6n_se_excite False\n600 block6n_project_conv False\n601 block6n_project_bn False\n602 block6n_drop False\n603 block6n_add False\n604 block6o_expand_conv True\n605 block6o_expand_bn True\n606 block6o_expand_activation True\n607 block6o_dwconv2 True\n608 block6o_bn True\n609 block6o_activation True\n610 block6o_se_squeeze True\n611 block6o_se_reshape True\n612 block6o_se_reduce True\n613 block6o_se_expand True\n614 block6o_se_excite True\n615 block6o_project_conv True\n616 block6o_project_bn True\n617 block6o_drop True\n618 block6o_add True\n619 block6p_expand_conv True\n620 block6p_expand_bn True\n621 block6p_expand_activation True\n622 block6p_dwconv2 True\n623 block6p_bn True\n624 block6p_activation True\n625 block6p_se_squeeze True\n626 block6p_se_reshape True\n627 block6p_se_reduce True\n628 block6p_se_expand True\n629 block6p_se_excite True\n630 block6p_project_conv True\n631 block6p_project_bn True\n632 block6p_drop True\n633 block6p_add True\n634 block6q_expand_conv True\n635 block6q_expand_bn True\n636 block6q_expand_activation True\n637 block6q_dwconv2 True\n638 block6q_bn True\n639 block6q_activation True\n640 block6q_se_squeeze True\n641 block6q_se_reshape True\n642 block6q_se_reduce True\n643 block6q_se_expand True\n644 block6q_se_excite True\n645 block6q_project_conv True\n646 block6q_project_bn True\n647 block6q_drop True\n648 block6q_add True\n649 block6r_expand_conv True\n650 block6r_expand_bn True\n651 block6r_expand_activation True\n652 block6r_dwconv2 True\n653 block6r_bn True\n654 block6r_activation True\n655 block6r_se_squeeze True\n656 block6r_se_reshape True\n657 block6r_se_reduce True\n658 block6r_se_expand True\n659 block6r_se_excite True\n660 block6r_project_conv True\n661 block6r_project_bn True\n662 block6r_drop True\n663 block6r_add True\n664 block7a_expand_conv True\n665 block7a_expand_bn True\n666 block7a_expand_activation True\n667 block7a_dwconv2 True\n668 block7a_bn True\n669 block7a_activation True\n670 block7a_se_squeeze True\n671 block7a_se_reshape True\n672 block7a_se_reduce True\n673 block7a_se_expand True\n674 block7a_se_excite True\n675 block7a_project_conv True\n676 block7a_project_bn True\n677 block7b_expand_conv True\n678 block7b_expand_bn True\n679 block7b_expand_activation True\n680 block7b_dwconv2 True\n681 block7b_bn True\n682 block7b_activation True\n683 block7b_se_squeeze True\n684 block7b_se_reshape True\n685 block7b_se_reduce True\n686 block7b_se_expand True\n687 block7b_se_excite True\n688 block7b_project_conv True\n689 block7b_project_bn True\n690 block7b_drop True\n691 block7b_add True\n692 block7c_expand_conv True\n693 block7c_expand_bn True\n694 block7c_expand_activation True\n695 block7c_dwconv2 True\n696 block7c_bn True\n697 block7c_activation True\n698 block7c_se_squeeze True\n699 block7c_se_reshape True\n700 block7c_se_reduce True\n701 block7c_se_expand True\n702 block7c_se_excite True\n703 block7c_project_conv True\n704 block7c_project_bn True\n705 block7c_drop True\n706 block7c_add True\n707 block7d_expand_conv True\n708 block7d_expand_bn True\n709 block7d_expand_activation True\n710 block7d_dwconv2 True\n711 block7d_bn True\n712 block7d_activation True\n713 block7d_se_squeeze True\n714 block7d_se_reshape True\n715 block7d_se_reduce True\n716 block7d_se_expand True\n717 block7d_se_excite True\n718 block7d_project_conv True\n719 block7d_project_bn True\n720 block7d_drop True\n721 block7d_add True\n722 block7e_expand_conv True\n723 block7e_expand_bn True\n724 block7e_expand_activation True\n725 block7e_dwconv2 True\n726 block7e_bn True\n727 block7e_activation True\n728 block7e_se_squeeze True\n729 block7e_se_reshape True\n730 block7e_se_reduce True\n731 block7e_se_expand True\n732 block7e_se_excite True\n733 block7e_project_conv True\n734 block7e_project_bn True\n735 block7e_drop True\n736 block7e_add True\n737 top_conv True\n738 top_bn True\n739 top_activation True\n740 avg_pool True\nModel: \"model\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_4 (InputLayer)        [(None, 96, 96, 3)]       0         \n                                                                 \n preprocessing (Sequential)  (None, 96, 96, 3)         0         \n                                                                 \n efficientnetv2-m (Functiona  (None, 1280)             53150388  \n l)                                                              \n                                                                 \n dense_3 (Dense)             (None, 64)                81984     \n                                                                 \n BatchNorm0 (BatchNormalizat  (None, 64)               256       \n ion)                                                            \n                                                                 \n dense_4 (Dense)             (None, 2)                 130       \n                                                                 \n=================================================================\nTotal params: 53,232,758\nTrainable params: 24,087,550\nNon-trainable params: 29,145,208\n_________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":"# Define optimizer, loss, and metrics\n# AdamW is an Adam optimizer which applies weight_decay to network layers,\n# i.e it's another way to apply l2 regularization to the whole network\noptimizer = tfk.optimizers.AdamW(1e-4, weight_decay=5e-4)\nloss = tfk.losses.CategoricalCrossentropy()\nmetrics = ['accuracy']\n\n# Compile the model with Categorical Cross-Entropy loss and Adam optimizer\nft_model.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n\n# Display model summary\nft_model.summary()","metadata":{"execution":{"iopub.status.busy":"2023-11-09T17:38:36.694864Z","iopub.execute_input":"2023-11-09T17:38:36.695778Z","iopub.status.idle":"2023-11-09T17:38:36.834998Z","shell.execute_reply.started":"2023-11-09T17:38:36.695742Z","shell.execute_reply":"2023-11-09T17:38:36.834110Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"Model: \"model\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_4 (InputLayer)        [(None, 96, 96, 3)]       0         \n                                                                 \n preprocessing (Sequential)  (None, 96, 96, 3)         0         \n                                                                 \n efficientnetv2-m (Functiona  (None, 1280)             53150388  \n l)                                                              \n                                                                 \n dense_3 (Dense)             (None, 64)                81984     \n                                                                 \n BatchNorm0 (BatchNormalizat  (None, 64)               256       \n ion)                                                            \n                                                                 \n dense_4 (Dense)             (None, 2)                 130       \n                                                                 \n=================================================================\nTotal params: 53,232,758\nTrainable params: 24,087,550\nNon-trainable params: 29,145,208\n_________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":"# Train the model\nft_history = ft_model.fit(\n    x = preprocess_input(X_train*255), # We need to apply the preprocessing thought for the MobileNetV2 network\n    y = y_train,\n    batch_size = 64,\n    epochs = 200,\n    validation_data = (preprocess_input(X_val*255), y_val), # We need to apply the preprocessing thought for the EfficientNetV2 network\n    callbacks = [tfk.callbacks.EarlyStopping(monitor='val_accuracy', mode='max', patience=20, restore_best_weights=True),\n                tfk.callbacks.ReduceLROnPlateau(monitor=\"val_accuracy\", factor=0.1, patience=20, min_lr=1e-5, mode='max')]\n).history","metadata":{"execution":{"iopub.status.busy":"2023-11-09T17:38:43.223173Z","iopub.execute_input":"2023-11-09T17:38:43.223551Z","iopub.status.idle":"2023-11-09T17:49:41.777422Z","shell.execute_reply.started":"2023-11-09T17:38:43.223521Z","shell.execute_reply":"2023-11-09T17:49:41.776499Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"Epoch 1/200\n62/62 [==============================] - 73s 266ms/step - loss: 0.5927 - accuracy: 0.7422 - val_loss: 0.5623 - val_accuracy: 0.7404 - lr: 1.0000e-04\nEpoch 2/200\n62/62 [==============================] - 10s 154ms/step - loss: 0.4643 - accuracy: 0.7725 - val_loss: 0.4301 - val_accuracy: 0.8019 - lr: 1.0000e-04\nEpoch 3/200\n62/62 [==============================] - 9s 153ms/step - loss: 0.4176 - accuracy: 0.8027 - val_loss: 0.3880 - val_accuracy: 0.8212 - lr: 1.0000e-04\nEpoch 4/200\n62/62 [==============================] - 10s 154ms/step - loss: 0.4016 - accuracy: 0.8252 - val_loss: 0.3584 - val_accuracy: 0.8423 - lr: 1.0000e-04\nEpoch 5/200\n62/62 [==============================] - 10s 155ms/step - loss: 0.3635 - accuracy: 0.8421 - val_loss: 0.3462 - val_accuracy: 0.8500 - lr: 1.0000e-04\nEpoch 6/200\n62/62 [==============================] - 9s 150ms/step - loss: 0.3396 - accuracy: 0.8517 - val_loss: 0.3492 - val_accuracy: 0.8481 - lr: 1.0000e-04\nEpoch 7/200\n62/62 [==============================] - 9s 150ms/step - loss: 0.3486 - accuracy: 0.8431 - val_loss: 0.3574 - val_accuracy: 0.8500 - lr: 1.0000e-04\nEpoch 8/200\n62/62 [==============================] - 9s 149ms/step - loss: 0.3178 - accuracy: 0.8587 - val_loss: 0.3404 - val_accuracy: 0.8481 - lr: 1.0000e-04\nEpoch 9/200\n62/62 [==============================] - 10s 155ms/step - loss: 0.3101 - accuracy: 0.8650 - val_loss: 0.3300 - val_accuracy: 0.8538 - lr: 1.0000e-04\nEpoch 10/200\n62/62 [==============================] - 9s 152ms/step - loss: 0.2944 - accuracy: 0.8749 - val_loss: 0.3357 - val_accuracy: 0.8615 - lr: 1.0000e-04\nEpoch 11/200\n62/62 [==============================] - 9s 147ms/step - loss: 0.2852 - accuracy: 0.8754 - val_loss: 0.3186 - val_accuracy: 0.8596 - lr: 1.0000e-04\nEpoch 12/200\n62/62 [==============================] - 9s 148ms/step - loss: 0.2749 - accuracy: 0.8787 - val_loss: 0.3211 - val_accuracy: 0.8538 - lr: 1.0000e-04\nEpoch 13/200\n62/62 [==============================] - 10s 154ms/step - loss: 0.2656 - accuracy: 0.8880 - val_loss: 0.3254 - val_accuracy: 0.8731 - lr: 1.0000e-04\nEpoch 14/200\n62/62 [==============================] - 9s 148ms/step - loss: 0.2518 - accuracy: 0.8973 - val_loss: 0.3261 - val_accuracy: 0.8673 - lr: 1.0000e-04\nEpoch 15/200\n62/62 [==============================] - 10s 155ms/step - loss: 0.2568 - accuracy: 0.8946 - val_loss: 0.3337 - val_accuracy: 0.8750 - lr: 1.0000e-04\nEpoch 16/200\n62/62 [==============================] - 10s 154ms/step - loss: 0.2496 - accuracy: 0.8943 - val_loss: 0.3347 - val_accuracy: 0.8769 - lr: 1.0000e-04\nEpoch 17/200\n62/62 [==============================] - 9s 149ms/step - loss: 0.2269 - accuracy: 0.9046 - val_loss: 0.3491 - val_accuracy: 0.8654 - lr: 1.0000e-04\nEpoch 18/200\n62/62 [==============================] - 9s 148ms/step - loss: 0.2397 - accuracy: 0.9021 - val_loss: 0.3090 - val_accuracy: 0.8673 - lr: 1.0000e-04\nEpoch 19/200\n62/62 [==============================] - 10s 154ms/step - loss: 0.2284 - accuracy: 0.9034 - val_loss: 0.3099 - val_accuracy: 0.8808 - lr: 1.0000e-04\nEpoch 20/200\n62/62 [==============================] - 9s 148ms/step - loss: 0.2105 - accuracy: 0.9112 - val_loss: 0.3658 - val_accuracy: 0.8577 - lr: 1.0000e-04\nEpoch 21/200\n62/62 [==============================] - 9s 147ms/step - loss: 0.2050 - accuracy: 0.9150 - val_loss: 0.3324 - val_accuracy: 0.8673 - lr: 1.0000e-04\nEpoch 22/200\n62/62 [==============================] - 9s 148ms/step - loss: 0.2089 - accuracy: 0.9109 - val_loss: 0.3729 - val_accuracy: 0.8615 - lr: 1.0000e-04\nEpoch 23/200\n62/62 [==============================] - 9s 154ms/step - loss: 0.1902 - accuracy: 0.9263 - val_loss: 0.3001 - val_accuracy: 0.8865 - lr: 1.0000e-04\nEpoch 24/200\n62/62 [==============================] - 9s 148ms/step - loss: 0.1870 - accuracy: 0.9226 - val_loss: 0.2953 - val_accuracy: 0.8865 - lr: 1.0000e-04\nEpoch 25/200\n62/62 [==============================] - 9s 148ms/step - loss: 0.1722 - accuracy: 0.9354 - val_loss: 0.3134 - val_accuracy: 0.8692 - lr: 1.0000e-04\nEpoch 26/200\n62/62 [==============================] - 9s 148ms/step - loss: 0.1868 - accuracy: 0.9253 - val_loss: 0.3324 - val_accuracy: 0.8673 - lr: 1.0000e-04\nEpoch 27/200\n62/62 [==============================] - 10s 154ms/step - loss: 0.1727 - accuracy: 0.9319 - val_loss: 0.3473 - val_accuracy: 0.8885 - lr: 1.0000e-04\nEpoch 28/200\n62/62 [==============================] - 9s 148ms/step - loss: 0.1717 - accuracy: 0.9304 - val_loss: 0.3578 - val_accuracy: 0.8750 - lr: 1.0000e-04\nEpoch 29/200\n62/62 [==============================] - 9s 148ms/step - loss: 0.1609 - accuracy: 0.9357 - val_loss: 0.3524 - val_accuracy: 0.8769 - lr: 1.0000e-04\nEpoch 30/200\n62/62 [==============================] - 9s 148ms/step - loss: 0.1684 - accuracy: 0.9316 - val_loss: 0.3379 - val_accuracy: 0.8788 - lr: 1.0000e-04\nEpoch 31/200\n62/62 [==============================] - 9s 148ms/step - loss: 0.1665 - accuracy: 0.9357 - val_loss: 0.3328 - val_accuracy: 0.8846 - lr: 1.0000e-04\nEpoch 32/200\n62/62 [==============================] - 9s 148ms/step - loss: 0.1563 - accuracy: 0.9384 - val_loss: 0.3662 - val_accuracy: 0.8731 - lr: 1.0000e-04\nEpoch 33/200\n62/62 [==============================] - 9s 148ms/step - loss: 0.1499 - accuracy: 0.9420 - val_loss: 0.3642 - val_accuracy: 0.8673 - lr: 1.0000e-04\nEpoch 34/200\n62/62 [==============================] - 9s 147ms/step - loss: 0.1484 - accuracy: 0.9435 - val_loss: 0.3356 - val_accuracy: 0.8769 - lr: 1.0000e-04\nEpoch 35/200\n62/62 [==============================] - 10s 156ms/step - loss: 0.1577 - accuracy: 0.9359 - val_loss: 0.3560 - val_accuracy: 0.8788 - lr: 1.0000e-04\nEpoch 36/200\n62/62 [==============================] - 9s 147ms/step - loss: 0.1487 - accuracy: 0.9430 - val_loss: 0.3398 - val_accuracy: 0.8808 - lr: 1.0000e-04\nEpoch 37/200\n62/62 [==============================] - 9s 147ms/step - loss: 0.1219 - accuracy: 0.9531 - val_loss: 0.4119 - val_accuracy: 0.8712 - lr: 1.0000e-04\nEpoch 38/200\n62/62 [==============================] - 9s 147ms/step - loss: 0.1408 - accuracy: 0.9453 - val_loss: 0.3887 - val_accuracy: 0.8750 - lr: 1.0000e-04\nEpoch 39/200\n62/62 [==============================] - 9s 148ms/step - loss: 0.1453 - accuracy: 0.9455 - val_loss: 0.3513 - val_accuracy: 0.8808 - lr: 1.0000e-04\nEpoch 40/200\n62/62 [==============================] - 9s 148ms/step - loss: 0.1318 - accuracy: 0.9468 - val_loss: 0.3802 - val_accuracy: 0.8808 - lr: 1.0000e-04\nEpoch 41/200\n62/62 [==============================] - 9s 148ms/step - loss: 0.1368 - accuracy: 0.9463 - val_loss: 0.3421 - val_accuracy: 0.8865 - lr: 1.0000e-04\nEpoch 42/200\n62/62 [==============================] - 10s 154ms/step - loss: 0.1361 - accuracy: 0.9463 - val_loss: 0.3081 - val_accuracy: 0.8942 - lr: 1.0000e-04\nEpoch 43/200\n62/62 [==============================] - 9s 148ms/step - loss: 0.1262 - accuracy: 0.9518 - val_loss: 0.3185 - val_accuracy: 0.8846 - lr: 1.0000e-04\nEpoch 44/200\n62/62 [==============================] - 10s 154ms/step - loss: 0.1158 - accuracy: 0.9521 - val_loss: 0.3359 - val_accuracy: 0.9019 - lr: 1.0000e-04\nEpoch 45/200\n62/62 [==============================] - 9s 147ms/step - loss: 0.1028 - accuracy: 0.9622 - val_loss: 0.3359 - val_accuracy: 0.8846 - lr: 1.0000e-04\nEpoch 46/200\n62/62 [==============================] - 9s 148ms/step - loss: 0.1117 - accuracy: 0.9546 - val_loss: 0.3558 - val_accuracy: 0.8846 - lr: 1.0000e-04\nEpoch 47/200\n62/62 [==============================] - 9s 148ms/step - loss: 0.1071 - accuracy: 0.9576 - val_loss: 0.3888 - val_accuracy: 0.8885 - lr: 1.0000e-04\nEpoch 48/200\n62/62 [==============================] - 9s 148ms/step - loss: 0.1296 - accuracy: 0.9478 - val_loss: 0.3666 - val_accuracy: 0.8788 - lr: 1.0000e-04\nEpoch 49/200\n62/62 [==============================] - 9s 148ms/step - loss: 0.1163 - accuracy: 0.9528 - val_loss: 0.3726 - val_accuracy: 0.8808 - lr: 1.0000e-04\nEpoch 50/200\n62/62 [==============================] - 9s 147ms/step - loss: 0.1162 - accuracy: 0.9556 - val_loss: 0.3290 - val_accuracy: 0.8942 - lr: 1.0000e-04\nEpoch 51/200\n62/62 [==============================] - 9s 148ms/step - loss: 0.1030 - accuracy: 0.9599 - val_loss: 0.3651 - val_accuracy: 0.8904 - lr: 1.0000e-04\nEpoch 52/200\n62/62 [==============================] - 9s 148ms/step - loss: 0.0972 - accuracy: 0.9622 - val_loss: 0.3631 - val_accuracy: 0.8923 - lr: 1.0000e-04\nEpoch 53/200\n62/62 [==============================] - 9s 148ms/step - loss: 0.1060 - accuracy: 0.9589 - val_loss: 0.3684 - val_accuracy: 0.8846 - lr: 1.0000e-04\nEpoch 54/200\n62/62 [==============================] - 9s 147ms/step - loss: 0.0989 - accuracy: 0.9629 - val_loss: 0.3984 - val_accuracy: 0.8615 - lr: 1.0000e-04\nEpoch 55/200\n62/62 [==============================] - 9s 148ms/step - loss: 0.0859 - accuracy: 0.9634 - val_loss: 0.4177 - val_accuracy: 0.8750 - lr: 1.0000e-04\nEpoch 56/200\n62/62 [==============================] - 9s 147ms/step - loss: 0.1169 - accuracy: 0.9556 - val_loss: 0.3994 - val_accuracy: 0.8692 - lr: 1.0000e-04\nEpoch 57/200\n62/62 [==============================] - 9s 147ms/step - loss: 0.0964 - accuracy: 0.9639 - val_loss: 0.3903 - val_accuracy: 0.8885 - lr: 1.0000e-04\nEpoch 58/200\n62/62 [==============================] - 9s 147ms/step - loss: 0.0943 - accuracy: 0.9637 - val_loss: 0.4133 - val_accuracy: 0.8769 - lr: 1.0000e-04\nEpoch 59/200\n62/62 [==============================] - 9s 148ms/step - loss: 0.0997 - accuracy: 0.9622 - val_loss: 0.3963 - val_accuracy: 0.8827 - lr: 1.0000e-04\nEpoch 60/200\n62/62 [==============================] - 9s 147ms/step - loss: 0.0943 - accuracy: 0.9672 - val_loss: 0.4278 - val_accuracy: 0.8750 - lr: 1.0000e-04\nEpoch 61/200\n62/62 [==============================] - 9s 147ms/step - loss: 0.0849 - accuracy: 0.9634 - val_loss: 0.3519 - val_accuracy: 0.8885 - lr: 1.0000e-04\nEpoch 62/200\n62/62 [==============================] - 9s 148ms/step - loss: 0.0832 - accuracy: 0.9692 - val_loss: 0.4183 - val_accuracy: 0.8827 - lr: 1.0000e-04\nEpoch 63/200\n62/62 [==============================] - 9s 148ms/step - loss: 0.0933 - accuracy: 0.9649 - val_loss: 0.4150 - val_accuracy: 0.8712 - lr: 1.0000e-04\nEpoch 64/200\n62/62 [==============================] - 10s 155ms/step - loss: 0.0920 - accuracy: 0.9634 - val_loss: 0.4059 - val_accuracy: 0.8885 - lr: 1.0000e-04\n","output_type":"stream"}]},{"cell_type":"code","source":"# Evaluate the model on the test set\ntest_accuracy = ft_model.evaluate(preprocess_input(X_test*255),y_test,verbose=0)[-1]\nprint('Test set accuracy %.4f' % test_accuracy)\n\nfrom sklearn.metrics import precision_score, recall_score\ny_pred = ft_model.predict(preprocess_input(X_test*255))\ny_pred = tf.argmax(y_pred, axis=-1)\ny_test_true = np.argmax(y_test, axis=-1)\n# Calculate precision and recall\nprecision = precision_score(y_test_true, y_pred)\nrecall = recall_score(y_test_true, y_pred)\n\n# Print the precision and recall\nprint(\"Precision:\", precision)\nprint(\"Recall:\", recall)","metadata":{"execution":{"iopub.status.busy":"2023-11-09T17:50:32.416249Z","iopub.execute_input":"2023-11-09T17:50:32.416660Z","iopub.status.idle":"2023-11-09T17:50:39.761838Z","shell.execute_reply.started":"2023-11-09T17:50:32.416627Z","shell.execute_reply":"2023-11-09T17:50:39.760941Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"Test set accuracy 0.8923\n17/17 [==============================] - 5s 56ms/step\nPrecision: 0.8480392156862745\nRecall: 0.8737373737373737\n","output_type":"stream"}]},{"cell_type":"code","source":"# Save the best model\nft_model.save('EFFICIENTNET-AUGGGG-FC-BATCHNORM-FT-')","metadata":{"execution":{"iopub.status.busy":"2023-11-09T17:11:33.474778Z","iopub.execute_input":"2023-11-09T17:11:33.475865Z","iopub.status.idle":"2023-11-09T17:14:54.943135Z","shell.execute_reply.started":"2023-11-09T17:11:33.475829Z","shell.execute_reply":"2023-11-09T17:14:54.942203Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"!zip -r EFFICIENTNET-AUGGGG-FC-BATCHNORM-FT.zip /kaggle/working/EFFICIENTNET-AUGGGG-FC-BATCHNORM-FT-","metadata":{"execution":{"iopub.status.busy":"2023-11-09T17:18:34.567058Z","iopub.execute_input":"2023-11-09T17:18:34.567475Z","iopub.status.idle":"2023-11-09T17:18:56.178383Z","shell.execute_reply.started":"2023-11-09T17:18:34.567441Z","shell.execute_reply":"2023-11-09T17:18:56.177170Z"},"trusted":true},"execution_count":35,"outputs":[{"name":"stdout","text":"  adding: kaggle/working/EFFICIENTNET-AUGGGG-FC-BATCHNORM-FT-/ (stored 0%)\n  adding: kaggle/working/EFFICIENTNET-AUGGGG-FC-BATCHNORM-FT-/keras_metadata.pb (deflated 96%)\n  adding: kaggle/working/EFFICIENTNET-AUGGGG-FC-BATCHNORM-FT-/variables/ (stored 0%)\n  adding: kaggle/working/EFFICIENTNET-AUGGGG-FC-BATCHNORM-FT-/variables/variables.data-00000-of-00001 (deflated 9%)\n  adding: kaggle/working/EFFICIENTNET-AUGGGG-FC-BATCHNORM-FT-/variables/variables.index (deflated 77%)\n  adding: kaggle/working/EFFICIENTNET-AUGGGG-FC-BATCHNORM-FT-/saved_model.pb (deflated 92%)\n  adding: kaggle/working/EFFICIENTNET-AUGGGG-FC-BATCHNORM-FT-/assets/ (stored 0%)\n  adding: kaggle/working/EFFICIENTNET-AUGGGG-FC-BATCHNORM-FT-/fingerprint.pb (stored 0%)\n","output_type":"stream"}]},{"cell_type":"code","source":"import shutil\nshutil.rmtree(\"/kaggle/working/EFFICIENTNET-AUGGGG-FC-BATCHNORM-FT-\")","metadata":{"execution":{"iopub.status.busy":"2023-11-09T16:50:53.498334Z","iopub.execute_input":"2023-11-09T16:50:53.499200Z","iopub.status.idle":"2023-11-09T16:50:53.557813Z","shell.execute_reply.started":"2023-11-09T16:50:53.499168Z","shell.execute_reply":"2023-11-09T16:50:53.557004Z"},"trusted":true},"execution_count":25,"outputs":[]}]}